## Launch
- select and load the llm as a judge for example2
- add the ability to run a generate of the compiled prompt
- ability to load an existing prompt back into the interface to re-run
- update for the latest DSPy version / experimental features

## Backlog
- support all the various optimizers and modules in the interface: 
    - ProgramOfThought, ReAct, MultiChainComparison, majority
    - BootstrapFineTune, Ensemble, LabeledFewShot
- add RAG support: Retrieve, Retrieval Model Clients
- local running of LLM with olama, llama.cpp or lm studio
- remove none for optimizers and instead set a baseline